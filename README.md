# Deep Learning Fundamentals with Tensorflow

## ðŸ‘‹ Introduction

Welcome to my notebooks on Fundamentals of Deep Learning with TensorFlow. In this repository, you will find my notes on Fundamentals of Deep Learning in which I gathered the building blocks of most deep learning algorithms. I used mainly three resources for creating these notebooks, I took my notes from [GÃ©ron, A. (2022)]() and [Chollet, F. (2021)]() and combined them with [Deep Learning Specialization]() on Coursera. In each notebook, I also provide a bunch of new resources that I found helpful. These notebooks are more focus-on hands-on practice with also a little bit of theory provided. I think these notebooks would be most useful for those who have prior experience with the course on Coursera as well as those who have experience with the aforementioned books or some experience in deep learning. 

## ðŸ—Š Content

**Deep Learning Fundamentals 1:** The first notebook starts with introducing Perceptron and then moves on to the backpropagation and activation functions. Lastly, we implement a shallow neural network in which we experiment with the hidden layer size.

**Deep Learning Fundamentals 2:** In the second notebook, we continue our discussion on Shallow Neural Networks and implement a Neural Network with L number of hidden layers from scratch. Then we implement a similar model using Keras.

**Deep Learning Fundamentals 3:** In this notebook, we explore Tensorflow and Keras to learn about basic functionalities of these libraries. We then do some basic practices on MNIST and Covid Dataset. Please refer to my Covid Data Analysis Repository if you would like to see how I generated the data used in this notebook. Moreover, we also use a MLP on a basic regression task.

**Deep Learning Fundamentals 4:** In this notebook, we start discussing about an important problem: Vanishing/Exploring Gradient Problem. Afterwards, we move on to exploring some of ways to get on top of it such as Initialization and Gradient Checking.

**Deep Learning Fundamentals 5:** In the fifth notebook, we go on our discussion on vanishing/exploring gradient problem and talk about activation function saturation. We then explore commonly used activation functions and talk about Batch Normalization.

**Deep Learning Fundamentals 6:** In the sixth notebook, we explore commonly used regularization and optimization techniques in Deep Learning and implement these techniques from scratch.

**Deep Learning Fundamentals 7:** In this notebook, we start how to use the techniques we learnt in the last three notebooks using Tensorflow/Keras.



