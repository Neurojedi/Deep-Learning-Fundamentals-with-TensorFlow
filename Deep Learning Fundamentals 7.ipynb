{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e9b9b3",
   "metadata": {},
   "source": [
    "# Deep Learning Fundamentals 7 - Into the World of Tensorflow/Keras 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603267b4",
   "metadata": {},
   "source": [
    "In the seventh notebook, I will talk about how to use the techniques we covered in the previous three notebooks by using Tensorflow and Keras. Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c1d5df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700c45fb",
   "metadata": {},
   "source": [
    "# Exploring Initializers and Activation Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de3ce9",
   "metadata": {},
   "source": [
    "Previously, we implemented initializers and activation functions from scratch. Now we will use them with Keras for practice, let's get started with loading our beloved dataset, Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b2f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3112b2",
   "metadata": {},
   "source": [
    "How many initializers are implemented in Keras ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5a6add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[initializers for initializers in dir(keras.initializers) if not initializers.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff6990",
   "metadata": {},
   "source": [
    "By default, Keras uses Glorot initialization with a uniform distribution. Let's train a network usign ReLU and default initialization then also talk about some methods that we previously used but didn't talk about much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56031f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7044 - accuracy: 0.7687 - val_loss: 0.5176 - val_accuracy: 0.8196\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4817 - accuracy: 0.8320 - val_loss: 0.4689 - val_accuracy: 0.8322\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4381 - accuracy: 0.8460 - val_loss: 0.4297 - val_accuracy: 0.8478\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4111 - accuracy: 0.8548 - val_loss: 0.4054 - val_accuracy: 0.8594\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3913 - accuracy: 0.8625 - val_loss: 0.3782 - val_accuracy: 0.8702\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3765 - accuracy: 0.8661 - val_loss: 0.3896 - val_accuracy: 0.8678\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3624 - accuracy: 0.8721 - val_loss: 0.3762 - val_accuracy: 0.8730\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3505 - accuracy: 0.8758 - val_loss: 0.3718 - val_accuracy: 0.8684\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3403 - accuracy: 0.8795 - val_loss: 0.3464 - val_accuracy: 0.8760\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3310 - accuracy: 0.8817 - val_loss: 0.3370 - val_accuracy: 0.8816\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.Dense(100),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14eb71a",
   "metadata": {},
   "source": [
    "In the third notebook, I used Keras for training some basic networks but didn't talk much about layers, and main methods that we use such as `compile()` and `fit()`  I think now we covered more enough building blocks of deep learning to talk about these things. \n",
    "\n",
    "In the code above, we first defined the layers. This neural network is composed of dense layers (or a fully connected network). These layers are appropriate for 2-rank tensors of shape (samples, features). On the other hand, for tensors for different ranks we need to use different models, for instance, we can use LSTM layers, RNN layers, or 1D-CNN layers for sequence data which is stored in 3-rank tensors of shape (samples, timesteps, features). Moreover, Image data is stored in 4-rank tensors and generally processed by 2D convolution layers (Conv2D). Secondly, we use the `compile()` method and pass the optimizer, loss, and metric. I passed two of them as strings and one of them as an object (that's because I wanted to tune `learning_rate` argument), however, all of them got converted to  Python objects in the end. For instance, `loss=\"sparse_categorical_crossentropy\"` becomes `loss=tf.keras.losses.SparseCategoricalCrossentropy` once we run the code. Lastly, I call `fit()` method which implements the training loop itself.\n",
    "\n",
    "\n",
    "Additionally, there are two important methods that we frequently use \n",
    "1. `evaluate()`: This method is used when we want to compute the loss and metrics after training. If the model doesn't have a metric then only the loss gets returned.\n",
    "2. `predict()`: This method is used for inference. The data we pass will be itareted over the data and in return we will get a Numpy array of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f5c67",
   "metadata": {},
   "source": [
    "Now let's go on talking about initializations and see if He initialization will improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5db688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.6898 - accuracy: 0.7735 - val_loss: 0.4971 - val_accuracy: 0.8284\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4789 - accuracy: 0.8324 - val_loss: 0.4276 - val_accuracy: 0.8616\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4337 - accuracy: 0.8486 - val_loss: 0.4115 - val_accuracy: 0.8574\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4071 - accuracy: 0.8572 - val_loss: 0.3937 - val_accuracy: 0.8672\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3878 - accuracy: 0.8647 - val_loss: 0.3825 - val_accuracy: 0.8718\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3713 - accuracy: 0.8690 - val_loss: 0.3692 - val_accuracy: 0.8746\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3592 - accuracy: 0.8734 - val_loss: 0.3679 - val_accuracy: 0.8708\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3480 - accuracy: 0.8777 - val_loss: 0.3654 - val_accuracy: 0.8742\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3378 - accuracy: 0.8807 - val_loss: 0.3671 - val_accuracy: 0.8746\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3283 - accuracy: 0.8842 - val_loss: 0.3590 - val_accuracy: 0.8708\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"HeUniform\"),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"HeUniform\"),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6faa59",
   "metadata": {},
   "source": [
    "Looks like both initilization techniques work almost equally well in this case. How would the performance change if I used another activation function ? Let's use LeakyRelu with HeUniform initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8957b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.6862 - accuracy: 0.7712 - val_loss: 0.4970 - val_accuracy: 0.8360\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4866 - accuracy: 0.8297 - val_loss: 0.4651 - val_accuracy: 0.8368\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4453 - accuracy: 0.8442 - val_loss: 0.4161 - val_accuracy: 0.8532\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4208 - accuracy: 0.8531 - val_loss: 0.4173 - val_accuracy: 0.8540\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4041 - accuracy: 0.8572 - val_loss: 0.3925 - val_accuracy: 0.8644\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3898 - accuracy: 0.8635 - val_loss: 0.3932 - val_accuracy: 0.8582\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3768 - accuracy: 0.8680 - val_loss: 0.3831 - val_accuracy: 0.8688\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3678 - accuracy: 0.8693 - val_loss: 0.3677 - val_accuracy: 0.8702\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3581 - accuracy: 0.8727 - val_loss: 0.3745 - val_accuracy: 0.8726\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3505 - accuracy: 0.8756 - val_loss: 0.3623 - val_accuracy: 0.8778\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"HeUniform\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"HeUniform\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd184f",
   "metadata": {},
   "source": [
    "What about using PReLU ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d846e716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7040 - accuracy: 0.7659 - val_loss: 0.5021 - val_accuracy: 0.8308\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4835 - accuracy: 0.8311 - val_loss: 0.4598 - val_accuracy: 0.8408\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4380 - accuracy: 0.8457 - val_loss: 0.4095 - val_accuracy: 0.8608\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4123 - accuracy: 0.8551 - val_loss: 0.4177 - val_accuracy: 0.8566\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3913 - accuracy: 0.8621 - val_loss: 0.3989 - val_accuracy: 0.8630\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3736 - accuracy: 0.8677 - val_loss: 0.3980 - val_accuracy: 0.8626\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3596 - accuracy: 0.8716 - val_loss: 0.3912 - val_accuracy: 0.8642\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3485 - accuracy: 0.8765 - val_loss: 0.3588 - val_accuracy: 0.8746\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3383 - accuracy: 0.8794 - val_loss: 0.3549 - val_accuracy: 0.8750\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3286 - accuracy: 0.8834 - val_loss: 0.3716 - val_accuracy: 0.8666\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"HeUniform\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"HeUniform\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d5f969",
   "metadata": {},
   "source": [
    "Let's also use HeNormal initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae9d4273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6878 - accuracy: 0.7710 - val_loss: 0.4902 - val_accuracy: 0.8386\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4760 - accuracy: 0.8343 - val_loss: 0.4340 - val_accuracy: 0.8544\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4331 - accuracy: 0.8483 - val_loss: 0.4275 - val_accuracy: 0.8510\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4072 - accuracy: 0.8579 - val_loss: 0.3972 - val_accuracy: 0.8616\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3870 - accuracy: 0.8634 - val_loss: 0.3715 - val_accuracy: 0.8696\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3721 - accuracy: 0.8696 - val_loss: 0.3664 - val_accuracy: 0.8736\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3576 - accuracy: 0.8730 - val_loss: 0.3709 - val_accuracy: 0.8696\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3465 - accuracy: 0.8765 - val_loss: 0.3473 - val_accuracy: 0.8778\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3363 - accuracy: 0.8805 - val_loss: 0.3620 - val_accuracy: 0.8724\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3270 - accuracy: 0.8846 - val_loss: 0.4044 - val_accuracy: 0.8464\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"HeNormal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"HeNormal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1eefa",
   "metadata": {},
   "source": [
    "Looks like PReLU and HeNormal provided the best result so far. \n",
    "\n",
    "\n",
    "We can also tune our initializer by changing the scaling factor. We previously talked about that He initialization uses $fan_{in}$ we can actually change this and use $fan_{avg}$ as well by using `keras.initializers.VarianceScaling()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f66ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6727 - accuracy: 0.7755 - val_loss: 0.4817 - val_accuracy: 0.8424\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4755 - accuracy: 0.8349 - val_loss: 0.4686 - val_accuracy: 0.8336\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4310 - accuracy: 0.8495 - val_loss: 0.4056 - val_accuracy: 0.8608\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4057 - accuracy: 0.8575 - val_loss: 0.3870 - val_accuracy: 0.8700\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3844 - accuracy: 0.8660 - val_loss: 0.3795 - val_accuracy: 0.8696\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3696 - accuracy: 0.8701 - val_loss: 0.3960 - val_accuracy: 0.8628\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3557 - accuracy: 0.8757 - val_loss: 0.3560 - val_accuracy: 0.8774\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3431 - accuracy: 0.8799 - val_loss: 0.3669 - val_accuracy: 0.8708\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3343 - accuracy: 0.8812 - val_loss: 0.3493 - val_accuracy: 0.8800\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3252 - accuracy: 0.8839 - val_loss: 0.3377 - val_accuracy: 0.8842\n"
     ]
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='normal')\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=init),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=init),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a04c8",
   "metadata": {},
   "source": [
    "In all the examples above, we had almost similar results and didn't get any vanishing/exploiding gradient problem. However, we can see a different result when we are using a very dense neural network. Let's see how it happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58eb664",
   "metadata": {},
   "source": [
    "## Let's Vanish/Explode these Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df520d",
   "metadata": {},
   "source": [
    "Here, I will do something very basic. I will train two neural networks. In the first one I will add lots of layers by using a for loop. In the second one, I will train almost the same model with the exception that I will use a different activation function. You will see that changing just the activation function may improve our model extraordinarily in some cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a41e6d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 2.3027 - accuracy: 0.0990 - val_loss: 2.3028 - val_accuracy: 0.0980\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 2.3027 - accuracy: 0.0985 - val_loss: 2.3028 - val_accuracy: 0.0986\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 2.3027 - accuracy: 0.0994 - val_loss: 2.3027 - val_accuracy: 0.0914\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 2.3027 - accuracy: 0.0994 - val_loss: 2.3027 - val_accuracy: 0.0914\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 2.3027 - accuracy: 0.0987 - val_loss: 2.3027 - val_accuracy: 0.0914\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 2.3027 - accuracy: 0.0988 - val_loss: 2.3028 - val_accuracy: 0.0914\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 2.3027 - accuracy: 0.0991 - val_loss: 2.3029 - val_accuracy: 0.0986\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 2.3027 - accuracy: 0.0995 - val_loss: 2.3028 - val_accuracy: 0.0986\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 2.3027 - accuracy: 0.0990 - val_loss: 2.3028 - val_accuracy: 0.0980\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 2.3027 - accuracy: 0.0995 - val_loss: 2.3028 - val_accuracy: 0.0986\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, kernel_initializer='lecun_normal',activation='relu'))\n",
    "for layer in range(50):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='lecun_normal',activation='relu')) # adding 50 hidden layers.\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124ad3c",
   "metadata": {},
   "source": [
    "Previously we talked about a paper in which it is proposed that using SELU activation function and LeCun initialization will self-normalize our neural network (each layer will have the same mean and variance during training) and this will solve vanishing/exploding gradients problem. Let's use these proposed model and see if it improves our neural metwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4560cbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 1.5003 - accuracy: 0.4050 - val_loss: 0.8019 - val_accuracy: 0.6812\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.7328 - accuracy: 0.7153 - val_loss: 0.8693 - val_accuracy: 0.6672\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.6804 - accuracy: 0.7614 - val_loss: 0.5321 - val_accuracy: 0.8148\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.5514 - accuracy: 0.8084 - val_loss: 0.4932 - val_accuracy: 0.8326\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4874 - accuracy: 0.8314 - val_loss: 0.4818 - val_accuracy: 0.8234\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.4724 - accuracy: 0.8355 - val_loss: 0.4912 - val_accuracy: 0.8382\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4438 - accuracy: 0.8448 - val_loss: 0.4243 - val_accuracy: 0.8474\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.4178 - accuracy: 0.8536 - val_loss: 0.4411 - val_accuracy: 0.8508\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4023 - accuracy: 0.8569 - val_loss: 0.3900 - val_accuracy: 0.8640\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.3881 - accuracy: 0.8617 - val_loss: 0.3916 - val_accuracy: 0.8614\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, kernel_initializer='lecun_normal',activation='selu'))\n",
    "for layer in range(50):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='lecun_normal',activation='selu')) # adding 50 hidden layers.\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1579027",
   "metadata": {},
   "source": [
    "**It worked !!** In the first model, we suffered from vanishing/exploiding gradient problem, however, this wasn't the case in the second example. But there are still some point that we need to be careful about which are stated in [Géron, A. (2019)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/).\n",
    "* The self-normalizing property of the SELU activation function is easily broken: you cannot use ℓ<sub>1</sub> or ℓ<sub>2</sub> regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize). However, in practice it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions.\n",
    "\n",
    "However, if we don't use any technique that break the self-normalizing property. The function preserves this property for even very big neural networks. Moreover, by default, the hyperparameters of SELU  are tuned in a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b77ed",
   "metadata": {},
   "source": [
    "The below suggestion is also taken from [the book](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/):\n",
    "\n",
    "\n",
    "**Which activation function should you use for the hidden layers of your deep neural networks?** \n",
    "* Although your mileage will vary, in general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic. If the network’s architecture prevents it from self-normalizing, then ELU may perform better than SELU (since SELU is not smooth at z = 0). If you care a lot about runtime latency, then you may prefer leaky ReLU. If you don’t want to tweak yet another hyperparameter, you may use the default α values used by Keras (e.g., 0.3 for leaky ReLU). If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, such as RReLU if your network is overfitting or PReLU if you have a huge training set. That said, because ReLU is the most used activation function (by far), many libraries and hardware accelerators provide ReLU-specific optimizations; therefore, if speed is your priority, ReLU might still be the best choice - [Géron, A. (2019)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f751e718",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a2403",
   "metadata": {},
   "source": [
    "We previously talked about that if we add BN as the very first layer of our neural network we generally don't need to standardize our training set. Moreover, sometimes applying BN before the activation function works better (try both). Also layer before a BN layer doesn't need to have bias terms, therefore, it would be appropriate to use `use_bias=False` to avoid wasting parameters (This is actually because that in the mean subtraction step of batch normalization, the bias term will be canceled out since we are adding it to all neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9304133b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "860/860 [==============================] - 5s 6ms/step - loss: 0.6066 - accuracy: 0.7929 - val_loss: 0.4351 - val_accuracy: 0.8438\n",
      "Epoch 2/10\n",
      "860/860 [==============================] - 6s 7ms/step - loss: 0.4230 - accuracy: 0.8520 - val_loss: 0.3856 - val_accuracy: 0.8582\n",
      "Epoch 3/10\n",
      "860/860 [==============================] - 5s 6ms/step - loss: 0.3792 - accuracy: 0.8646 - val_loss: 0.3662 - val_accuracy: 0.8666\n",
      "Epoch 4/10\n",
      "860/860 [==============================] - 6s 7ms/step - loss: 0.3518 - accuracy: 0.8746 - val_loss: 0.3599 - val_accuracy: 0.8738\n",
      "Epoch 5/10\n",
      "860/860 [==============================] - 6s 7ms/step - loss: 0.3313 - accuracy: 0.8816 - val_loss: 0.3438 - val_accuracy: 0.8772\n",
      "Epoch 6/10\n",
      "860/860 [==============================] - 5s 5ms/step - loss: 0.3133 - accuracy: 0.8865 - val_loss: 0.3389 - val_accuracy: 0.8782\n",
      "Epoch 7/10\n",
      "860/860 [==============================] - 5s 5ms/step - loss: 0.2986 - accuracy: 0.8922 - val_loss: 0.3341 - val_accuracy: 0.8820\n",
      "Epoch 8/10\n",
      "860/860 [==============================] - 5s 6ms/step - loss: 0.2863 - accuracy: 0.8958 - val_loss: 0.3291 - val_accuracy: 0.8806\n",
      "Epoch 9/10\n",
      "860/860 [==============================] - 4s 5ms/step - loss: 0.2750 - accuracy: 0.9014 - val_loss: 0.3236 - val_accuracy: 0.8824\n",
      "Epoch 10/10\n",
      "860/860 [==============================] - 4s 5ms/step - loss: 0.2616 - accuracy: 0.9064 - val_loss: 0.3261 - val_accuracy: 0.8838\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"HeUniform\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"HeUniform\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986c3887",
   "metadata": {},
   "source": [
    "Let's see the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f999659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 300)               300       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "p_re_lu_7 (PReLU)            (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,746\n",
      "Trainable params: 269,378\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bd64b",
   "metadata": {},
   "source": [
    "Let's also look at the parameters BN layer added to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a91fc478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_1/gamma:0', True),\n",
       " ('batch_normalization_1/beta:0', True),\n",
       " ('batch_normalization_1/moving_mean:0', False),\n",
       " ('batch_normalization_1/moving_variance:0', False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[4].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a6478",
   "metadata": {},
   "source": [
    "Moving mean and moving variance are non-trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95945724",
   "metadata": {},
   "source": [
    "Moreover, the BatchNormalization class has some hyperparameters we can tune such as the momentum (the hyperparameter that is used to update the exponential moving averages given a new value v (i.e., a new vector of input means or standard deviations computed over the current batch). A good momentum should be generally close to 1. The other hyperparameter is an axis which determines the axis that should be normalized and it is -1 by default which corresponds that the last axis will be normalized.\n",
    "\n",
    "\n",
    "The below paragraph is taken from [the book](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
    "* When the input batch is 2D (i.e., the batch shape is [batch size, features]), this means that each input feature will be normalized based on the mean and standard deviation computed across all the instances in the batch. For example, the first BN layer in the previous code example will independently normalize (and rescale and shift) each of the 784 input features. If we move the first BN layer before the Flatten layer, then the input batches will be 3D, with shape [batch size, height, width]; therefore, the BN layer will compute 28 means and 28 standard deviations (1 per column of pixels, computed across all instances in the batch and across all rows in the column), and it will normalize all pixels in a given column using the same mean and standard deviation. There will also be just 28 scale parameters and 28 shift parameters. If instead you still want to treat each of the 784 pixels independently, then you should set axis=[1, 2]. Notice that the BN layer does not perform the same computation during training and after training: it uses batch statistics during training and the “final” statistics after training (i.e., the final values of the moving averages)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481f468",
   "metadata": {},
   "source": [
    "A good article for PReLU [link](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-prelu-with-keras.md)\n",
    "\n",
    "Important paper : [Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/abs/1901.09321)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f981179",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f78c7",
   "metadata": {},
   "source": [
    "Let's use the model that gave us the best scores with different optimization functions to see whether we can improve it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c95b8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Optimizer_tryout(selected):\n",
    "    optimizer = selected\n",
    "    model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"HeUniform\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"HeUniform\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=10,\n",
    "                      validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f507125",
   "metadata": {},
   "source": [
    "## Momentum optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bfc0f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5652 - accuracy: 0.8026 - val_loss: 0.3900 - val_accuracy: 0.8670\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4089 - accuracy: 0.8535 - val_loss: 0.3573 - val_accuracy: 0.8722\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.3674 - accuracy: 0.8677 - val_loss: 0.3354 - val_accuracy: 0.8822\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3407 - accuracy: 0.8763 - val_loss: 0.3236 - val_accuracy: 0.8884\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3205 - accuracy: 0.8851 - val_loss: 0.3163 - val_accuracy: 0.8910\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3032 - accuracy: 0.8903 - val_loss: 0.3166 - val_accuracy: 0.8908\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2884 - accuracy: 0.8956 - val_loss: 0.3093 - val_accuracy: 0.8884\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2765 - accuracy: 0.8999 - val_loss: 0.3040 - val_accuracy: 0.8932\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2659 - accuracy: 0.9025 - val_loss: 0.3081 - val_accuracy: 0.8922\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2547 - accuracy: 0.9061 - val_loss: 0.3093 - val_accuracy: 0.8886\n"
     ]
    }
   ],
   "source": [
    "Optimizer_tryout(keras.optimizers.SGD(lr=0.001, momentum=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc24854",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26e69e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5726 - accuracy: 0.7994 - val_loss: 0.3951 - val_accuracy: 0.8658\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4106 - accuracy: 0.8529 - val_loss: 0.3604 - val_accuracy: 0.8732\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3665 - accuracy: 0.8684 - val_loss: 0.3418 - val_accuracy: 0.8800\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3424 - accuracy: 0.8774 - val_loss: 0.3244 - val_accuracy: 0.8846\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3191 - accuracy: 0.8851 - val_loss: 0.3204 - val_accuracy: 0.8852\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3038 - accuracy: 0.8907 - val_loss: 0.3186 - val_accuracy: 0.8900\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2883 - accuracy: 0.8956 - val_loss: 0.3082 - val_accuracy: 0.8882\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2738 - accuracy: 0.9003 - val_loss: 0.3106 - val_accuracy: 0.8876\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2645 - accuracy: 0.9028 - val_loss: 0.3162 - val_accuracy: 0.8854\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2542 - accuracy: 0.9069 - val_loss: 0.3096 - val_accuracy: 0.8902\n"
     ]
    }
   ],
   "source": [
    "Optimizer_tryout(keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e30af",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d40d023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.7339 - accuracy: 0.7568 - val_loss: 0.5051 - val_accuracy: 0.8318\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5321 - accuracy: 0.8175 - val_loss: 0.4483 - val_accuracy: 0.8484\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.4868 - accuracy: 0.8317 - val_loss: 0.4208 - val_accuracy: 0.8556\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4606 - accuracy: 0.8415 - val_loss: 0.4010 - val_accuracy: 0.8618\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4403 - accuracy: 0.8480 - val_loss: 0.3928 - val_accuracy: 0.8648\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4279 - accuracy: 0.8523 - val_loss: 0.3836 - val_accuracy: 0.8680\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4157 - accuracy: 0.8550 - val_loss: 0.3761 - val_accuracy: 0.8696\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4102 - accuracy: 0.8569 - val_loss: 0.3696 - val_accuracy: 0.8698\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4021 - accuracy: 0.8595 - val_loss: 0.3653 - val_accuracy: 0.8716\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3901 - accuracy: 0.8639 - val_loss: 0.3599 - val_accuracy: 0.8732\n"
     ]
    }
   ],
   "source": [
    "Optimizer_tryout(keras.optimizers.Adagrad(lr=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6004fb0",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08d0796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 0.4721 - accuracy: 0.8307 - val_loss: 0.3514 - val_accuracy: 0.8730\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 0.3623 - accuracy: 0.8690 - val_loss: 0.3202 - val_accuracy: 0.8862\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.3228 - accuracy: 0.8820 - val_loss: 0.3132 - val_accuracy: 0.8886\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.2969 - accuracy: 0.8910 - val_loss: 0.2999 - val_accuracy: 0.8922\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2777 - accuracy: 0.8981 - val_loss: 0.3019 - val_accuracy: 0.8952\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2582 - accuracy: 0.9049 - val_loss: 0.3106 - val_accuracy: 0.8926\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2444 - accuracy: 0.9102 - val_loss: 0.2904 - val_accuracy: 0.8988\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2352 - accuracy: 0.9142 - val_loss: 0.3024 - val_accuracy: 0.8964\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2246 - accuracy: 0.9178 - val_loss: 0.3281 - val_accuracy: 0.8878\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2125 - accuracy: 0.9226 - val_loss: 0.3150 - val_accuracy: 0.8952\n"
     ]
    }
   ],
   "source": [
    "Optimizer_tryout(keras.optimizers.RMSprop(lr=0.001, rho=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b315dc0",
   "metadata": {},
   "source": [
    "## Adam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de6b9d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.4721 - accuracy: 0.8302 - val_loss: 0.3586 - val_accuracy: 0.8704\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.3635 - accuracy: 0.8667 - val_loss: 0.3120 - val_accuracy: 0.8816\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3230 - accuracy: 0.8811 - val_loss: 0.2984 - val_accuracy: 0.8890\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2968 - accuracy: 0.8901 - val_loss: 0.3109 - val_accuracy: 0.8892\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2815 - accuracy: 0.8952 - val_loss: 0.3128 - val_accuracy: 0.8832\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2598 - accuracy: 0.9032 - val_loss: 0.3171 - val_accuracy: 0.8878\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2446 - accuracy: 0.9078 - val_loss: 0.3008 - val_accuracy: 0.8936\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2300 - accuracy: 0.9139 - val_loss: 0.3110 - val_accuracy: 0.8850\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2194 - accuracy: 0.9168 - val_loss: 0.2925 - val_accuracy: 0.9008\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2074 - accuracy: 0.9221 - val_loss: 0.3125 - val_accuracy: 0.8924\n"
     ]
    }
   ],
   "source": [
    "Optimizer_tryout(keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af7e255",
   "metadata": {},
   "source": [
    "# Adam Types\n",
    "\n",
    "1. Adamax: This version of Adam is proposed in the same paper that Adam is proposed. The Adam optimization algorithm scales down the parameter updates by $L2$ norm of the time-decayed gradient, on the other hand, Adamax uses $L_{max}$ norm and scales the parameter updates by the max of the time-decayed gradients. In theory this adjusment makes Adamax more stable but it is up to the dataset in practice.\n",
    "\n",
    "\n",
    "2. Nadam: Nadam algorithm uses Nesterov trick with Adam optimization which makes it converge slightly faster than Adam. In the [paper](https://cs229.stanford.edu/proj2015/054_report.pdf) that algorithm is proposed, it is also reported that this algorithm generally outperform Adam but at the same time sometimes outperformed by RMSProp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e1197",
   "metadata": {},
   "source": [
    "## Adamax Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7188c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4845 - accuracy: 0.8279 - val_loss: 0.3641 - val_accuracy: 0.8718\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.3635 - accuracy: 0.8686 - val_loss: 0.3253 - val_accuracy: 0.8820\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3226 - accuracy: 0.8824 - val_loss: 0.3111 - val_accuracy: 0.8844\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2930 - accuracy: 0.8928 - val_loss: 0.3053 - val_accuracy: 0.8904\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2716 - accuracy: 0.9013 - val_loss: 0.3016 - val_accuracy: 0.8904\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2519 - accuracy: 0.9068 - val_loss: 0.2903 - val_accuracy: 0.8944\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2382 - accuracy: 0.9123 - val_loss: 0.2937 - val_accuracy: 0.8960\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2217 - accuracy: 0.9165 - val_loss: 0.2868 - val_accuracy: 0.8974\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2121 - accuracy: 0.9207 - val_loss: 0.2946 - val_accuracy: 0.8910\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2020 - accuracy: 0.9256 - val_loss: 0.2904 - val_accuracy: 0.9004\n"
     ]
    }
   ],
   "source": [
    "Optimizer_tryout(keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10384ab",
   "metadata": {},
   "source": [
    "## Nadam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc87d6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.4681 - accuracy: 0.8333 - val_loss: 0.3441 - val_accuracy: 0.8780\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.3574 - accuracy: 0.8683 - val_loss: 0.3280 - val_accuracy: 0.8818\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 20s 11ms/step - loss: 0.3219 - accuracy: 0.8815 - val_loss: 0.3115 - val_accuracy: 0.8894\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.2955 - accuracy: 0.8899 - val_loss: 0.2968 - val_accuracy: 0.8944\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2743 - accuracy: 0.8962 - val_loss: 0.3010 - val_accuracy: 0.8894\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 20s 11ms/step - loss: 0.2584 - accuracy: 0.9025 - val_loss: 0.2969 - val_accuracy: 0.8908\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.2446 - accuracy: 0.9081 - val_loss: 0.2858 - val_accuracy: 0.8936\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.2317 - accuracy: 0.9117 - val_loss: 0.2965 - val_accuracy: 0.8936\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.2175 - accuracy: 0.9190 - val_loss: 0.3003 - val_accuracy: 0.8964\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.2093 - accuracy: 0.9218 - val_loss: 0.2999 - val_accuracy: 0.8966\n"
     ]
    }
   ],
   "source": [
    "Optimizer_tryout(keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0f7e01",
   "metadata": {},
   "source": [
    "Also see these versions of Adam\n",
    "1. [Adagrad](https://keras.io/api/optimizers/adagrad/)\n",
    "2. [Adadelta](https://keras.io/api/optimizers/adadelta/)\n",
    "\n",
    "**Notes from [the book](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/):**\n",
    "\n",
    "* Adaptive optimization methods (including RMSProp, Adam, and Nadam optimization) are often great, converging fast to a good solution. However, a 2017 [paper](https://arxiv.org/abs/1705.08292) by Ashia C. Wilson et al. showed that they can lead to solutions that generalize poorly on some datasets. So when you are disappointed by your model’s performance, try using plain Nesterov Accelerated Gradient instead: your dataset may just be allergic to adaptive gradients. Also check out the latest research, because it’s moving fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8471c",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4b3c5",
   "metadata": {},
   "source": [
    "Let's add L2-norm to our neural network and see how it affects the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ffbdd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 20s 11ms/step - loss: 1.2700 - accuracy: 0.8011 - val_loss: 0.7218 - val_accuracy: 0.8214\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.7316 - accuracy: 0.8155 - val_loss: 0.6579 - val_accuracy: 0.8376\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 20s 12ms/step - loss: 0.6688 - accuracy: 0.8267 - val_loss: 0.5892 - val_accuracy: 0.8548\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.6204 - accuracy: 0.8346 - val_loss: 0.5404 - val_accuracy: 0.8578\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 0.5950 - accuracy: 0.8375 - val_loss: 0.5519 - val_accuracy: 0.8530\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 21s 12ms/step - loss: 0.5725 - accuracy: 0.8412 - val_loss: 0.5199 - val_accuracy: 0.8594\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 21s 12ms/step - loss: 0.5636 - accuracy: 0.8423 - val_loss: 0.5265 - val_accuracy: 0.8612\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.5502 - accuracy: 0.8455 - val_loss: 0.4956 - val_accuracy: 0.8586\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 25s 14ms/step - loss: 0.5463 - accuracy: 0.8445 - val_loss: 0.5421 - val_accuracy: 0.8460\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.5406 - accuracy: 0.8444 - val_loss: 0.5138 - val_accuracy: 0.8546\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False,kernel_regularizer='l2'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False,kernel_regularizer='l2'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\",kernel_regularizer='l2')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099232d",
   "metadata": {},
   "source": [
    "Now I will use L1-norm but I will pass the argument as Python Object because I want to tune the penality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cde91f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 6.1403 - accuracy: 0.7226 - val_loss: 1.8012 - val_accuracy: 0.8022\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 1.7472 - accuracy: 0.7788 - val_loss: 1.4452 - val_accuracy: 0.8248\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 1.5018 - accuracy: 0.7910 - val_loss: 1.3863 - val_accuracy: 0.8002\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 1.3621 - accuracy: 0.7981 - val_loss: 1.2265 - val_accuracy: 0.8300\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 1.2634 - accuracy: 0.8017 - val_loss: 1.1502 - val_accuracy: 0.8334\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 1.1938 - accuracy: 0.8060 - val_loss: 1.1017 - val_accuracy: 0.8332\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 1.1490 - accuracy: 0.8057 - val_loss: 1.0920 - val_accuracy: 0.8236\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 1.1079 - accuracy: 0.8070 - val_loss: 1.0097 - val_accuracy: 0.8360\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 1.0764 - accuracy: 0.8083 - val_loss: 0.9936 - val_accuracy: 0.8264\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 22s 13ms/step - loss: 1.0616 - accuracy: 0.8089 - val_loss: 1.0164 - val_accuracy: 0.8224\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l1(0.01)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l1(0.01)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\",\n",
    "                       kernel_regularizer=keras.regularizers.l1(0.01))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed9f63",
   "metadata": {},
   "source": [
    "Instead of defining the activation function, initializer and regularizer in each layer we can use `functools.partial()` function to create a wrapper and call it with arguments we defined. I will create the wrapper with elastic net term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c37c5299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 22s 13ms/step - loss: 6.4042 - accuracy: 0.7169 - val_loss: 1.8937 - val_accuracy: 0.7830\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 21s 12ms/step - loss: 1.8271 - accuracy: 0.7726 - val_loss: 1.7095 - val_accuracy: 0.7910\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 20s 11ms/step - loss: 1.5268 - accuracy: 0.7892 - val_loss: 1.3467 - val_accuracy: 0.8176\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 1.3808 - accuracy: 0.7929 - val_loss: 1.3027 - val_accuracy: 0.8060\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 1.2872 - accuracy: 0.7968 - val_loss: 1.1993 - val_accuracy: 0.8222\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 1.2124 - accuracy: 0.8000 - val_loss: 1.1425 - val_accuracy: 0.8210\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 20s 12ms/step - loss: 1.1643 - accuracy: 0.8017 - val_loss: 1.0901 - val_accuracy: 0.8136\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 24s 14ms/step - loss: 1.1252 - accuracy: 0.8029 - val_loss: 1.0983 - val_accuracy: 0.8070\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 1.1041 - accuracy: 0.8025 - val_loss: 1.0317 - val_accuracy: 0.8274\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 22s 13ms/step - loss: 1.0839 - accuracy: 0.8037 - val_loss: 1.0298 - val_accuracy: 0.8174\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"selu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l1_l2(l1=0.01,l2=0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    RegularizedDense(300),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    RegularizedDense(100),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6e67b",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe99a55f",
   "metadata": {},
   "source": [
    "Now let's implement Dropout regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e0de09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.6017 - accuracy: 0.7844 - val_loss: 0.3849 - val_accuracy: 0.8630\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.4751 - accuracy: 0.8258 - val_loss: 0.3612 - val_accuracy: 0.8694\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.4409 - accuracy: 0.8391 - val_loss: 0.3324 - val_accuracy: 0.8724\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 21s 12ms/step - loss: 0.4189 - accuracy: 0.8452 - val_loss: 0.3276 - val_accuracy: 0.8832\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.4030 - accuracy: 0.8510 - val_loss: 0.3076 - val_accuracy: 0.8824\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.3943 - accuracy: 0.8555 - val_loss: 0.3058 - val_accuracy: 0.8832\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 20s 11ms/step - loss: 0.3856 - accuracy: 0.8561 - val_loss: 0.3056 - val_accuracy: 0.8838\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 18s 11ms/step - loss: 0.3764 - accuracy: 0.8611 - val_loss: 0.2947 - val_accuracy: 0.8900\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.3699 - accuracy: 0.8610 - val_loss: 0.2963 - val_accuracy: 0.8872\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.3626 - accuracy: 0.8654 - val_loss: 0.2951 - val_accuracy: 0.8880\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.25),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.25),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.25),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e90866",
   "metadata": {},
   "source": [
    "I used BatchNorm after Dropout which is because using it before may cause information leakage.\n",
    "\n",
    "**Notes from [the book](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/):**\n",
    "\n",
    "* Since dropout is only active during training, comparing the training loss and the validation loss can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So make sure to evaluate the training loss without dropout (e.g., after training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500dc4b",
   "metadata": {},
   "source": [
    "## Alpha Dropout\n",
    "\n",
    "One problem of regular dropout is that it breaks self-normalization feature of SELU + Lecun_normalization combination. To cope with that, we can use alpha dropout which preserves the mean and standard deviation of its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48d33a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 1.0597 - accuracy: 0.6018 - val_loss: 0.9336 - val_accuracy: 0.7078\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.9141 - accuracy: 0.6573 - val_loss: 0.8956 - val_accuracy: 0.7246\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.8861 - accuracy: 0.6666 - val_loss: 0.9292 - val_accuracy: 0.6994\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.8689 - accuracy: 0.6739 - val_loss: 1.0638 - val_accuracy: 0.6956\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.8586 - accuracy: 0.6782 - val_loss: 0.9549 - val_accuracy: 0.7108\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.8385 - accuracy: 0.6829 - val_loss: 0.8677 - val_accuracy: 0.7532\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.8334 - accuracy: 0.6857 - val_loss: 0.7209 - val_accuracy: 0.7664\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.8224 - accuracy: 0.6903 - val_loss: 0.8048 - val_accuracy: 0.7554\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - ETA: 0s - loss: 0.8135 - accuracy: 0.68 - 6s 3ms/step - loss: 0.8139 - accuracy: 0.6883 - val_loss: 0.8013 - val_accuracy: 0.7526\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.8087 - accuracy: 0.6933 - val_loss: 0.8355 - val_accuracy: 0.7334\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e9d43",
   "metadata": {},
   "source": [
    "## Monte Carlo Dropout - A free lunch ?\n",
    "\n",
    "An interesting technique called Monte Carlo Dropout published in a [paper](https://arxiv.org/abs/1506.02142) by Yarin Gal and Zoubin Ghahramani states that there is a good connection between dropout networks and Deep Gaussian Process (An approximate bayesian inference method). When we use dropout in each iteration, we get a slightly different neural network architecture. While doing predictions for test data we can use these different architectures and get the average prediction results which will cause our model to predict slightly better. You can read more about this algorithm in this [article](https://towardsdatascience.com/monte-carlo-dropout-7fd52f8b6571). The algorithm resembles a voting classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a6a49",
   "metadata": {},
   "source": [
    "Let's make 100 predictions over the test set (we need to set `training=True` to ensure that the Dropout layer is active) and then we stack the predictions. Thanks to dropout predictions are made by different arhitectures, in other words, we have different predictions. When we average over the first dimension (axis=0) we will get a array of shape that we would get with a single prediction. In the end, averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single prediction with dropout off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "902a2f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e59e41",
   "metadata": {},
   "source": [
    "Let's look at the prediction for the first instance in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39c4c24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test[:1]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db6ec5",
   "metadata": {},
   "source": [
    "The model says that that instance belong to class 9 with %97 probability. Let's see how this probability will change when we use Monte Carlo Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56140af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.round(y_probas[:, :1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989ae5c",
   "metadata": {},
   "source": [
    "In the end, the model still says that this instance belong to the class 9. However, it is more unsure about the class of instance. Let's average over the first dimension to get the classification result for the first instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ca4a4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.2 , 0.04, 0.45]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d35267",
   "metadata": {},
   "source": [
    "We can also have a look at the standard deviation of the probability estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49271e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02, 0.01, 0.  , 0.02, 0.  , 0.21, 0.02, 0.14, 0.08, 0.25]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f40f190",
   "metadata": {},
   "source": [
    "Looks like there is a big variance in the probability estimation. It is not that important for this model but it may be very important especially while trying to build a model for medical prediction. Let's also see whether we improved the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "876bbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ceba718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7792"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda37c5",
   "metadata": {},
   "source": [
    "Looks like we improved the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21126ce8",
   "metadata": {},
   "source": [
    "*  The number of Monte Carlo samples you use (100 in this example) is a hyperparameter you can tweak. The higher it is, the more accurate the predictions and their uncertainty estimates will be. However, if you double it, inference time will also be doubled. Moreover, above a certain number of samples, you will notice little improvement. So your job is to find the right trade-off between latency and accuracy, depending on your application - [Géron, A. (2019)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee899098",
   "metadata": {},
   "source": [
    "It is not a good idea to force training mode if we have special layers like BatchNormalization layers. In cases like that we can use Subclass API and override the call() method to directly force its training argument. We can also do the same thing for Alpha Dropout and get a MCAlpha Dropout version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "068fdb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fbd29",
   "metadata": {},
   "source": [
    "Let's use MCAlphaDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49679d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04675e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_20 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout (MCAlphaDro (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout_1 (MCAlphaD (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout_2 (MCAlphaD (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6adc775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab791382",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83234b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.002, 0.   , 0.001, 0.001, 0.002, 0.339, 0.003, 0.207, 0.025,\n",
       "        0.42 ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean([mc_model.predict(X_test[:1]) for sample in range(100)], axis=0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a544fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([mc_model(X_test) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82ceb520",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9638ca3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7803"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ce4ba",
   "metadata": {},
   "source": [
    "Another way to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6355d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(np.round(np.mean([mc_model.predict(X_test) for sample in range(100)], axis=0), 3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78ff0ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7806"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b397862",
   "metadata": {},
   "source": [
    "This MCDropout class will work with all Keras APIs, including the Sequential API. If you only care about the\n",
    "Functional API or the Subclassing API, you do not have to create an MCDropout class; you can create a regular\n",
    "Dropout layer and call it with `training=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7566bf5",
   "metadata": {},
   "source": [
    "## Max Norm Regularization\n",
    "\n",
    "Max-Norm Regularization or Max-Norm Constraint is a popular regularization technique in which we constrain the weights of connections for each neuron. It uses L2 term even though it does not actually add the term into the overall loss function. Instead, after each training step the weight vector is forced to have L2 norm if it is less than or equal to the hyperparameter **r**. If this condition is not satisfied, the weight vector is replaced with by the unit vector that is scaled by r. \n",
    "\n",
    "You can find more about Max-Norm Regularization in the following articles.\n",
    "\n",
    "[This one is about Max-Norm Regularization](https://machinelearningjourney.com/index.php/2021/01/15/max-norm-regularization/)  \n",
    "\n",
    "[This one contains information about all the regularization methods](https://cs231n.github.io/neural-networks-2/#reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a5ece02",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                           kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b46b69ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.5042 - accuracy: 0.8164 - val_loss: 0.4153 - val_accuracy: 0.8446\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.3976 - accuracy: 0.8552 - val_loss: 0.3892 - val_accuracy: 0.8576\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 16s 10ms/step - loss: 0.3722 - accuracy: 0.8634 - val_loss: 0.4403 - val_accuracy: 0.8346\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.3626 - accuracy: 0.8662 - val_loss: 0.3684 - val_accuracy: 0.8662\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 15s 8ms/step - loss: 0.3546 - accuracy: 0.8676 - val_loss: 0.3409 - val_accuracy: 0.8764\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.3526 - accuracy: 0.8683 - val_loss: 0.3670 - val_accuracy: 0.8572\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3480 - accuracy: 0.8705 - val_loss: 0.3717 - val_accuracy: 0.8660\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3495 - accuracy: 0.8704 - val_loss: 0.3947 - val_accuracy: 0.8576\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.3472 - accuracy: 0.8697 - val_loss: 0.3542 - val_accuracy: 0.8684\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3484 - accuracy: 0.8705 - val_loss: 0.3523 - val_accuracy: 0.8688\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.3442 - accuracy: 0.8720 - val_loss: 0.3434 - val_accuracy: 0.8744\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.3443 - accuracy: 0.8730 - val_loss: 0.3584 - val_accuracy: 0.8692\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.3443 - accuracy: 0.8719 - val_loss: 0.3884 - val_accuracy: 0.8510\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.3454 - accuracy: 0.8730 - val_loss: 0.3586 - val_accuracy: 0.8698\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.3442 - accuracy: 0.8719 - val_loss: 0.3717 - val_accuracy: 0.8638\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 15s 8ms/step - loss: 0.3453 - accuracy: 0.8715 - val_loss: 0.3844 - val_accuracy: 0.8484\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.3442 - accuracy: 0.8714 - val_loss: 0.3399 - val_accuracy: 0.8806\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3443 - accuracy: 0.8713 - val_loss: 0.3546 - val_accuracy: 0.8686\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 0.3407 - accuracy: 0.8731 - val_loss: 0.3588 - val_accuracy: 0.8710\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.3434 - accuracy: 0.8724 - val_loss: 0.3618 - val_accuracy: 0.8668\n"
     ]
    }
   ],
   "source": [
    "MaxNormDense = partial(keras.layers.Dense,\n",
    "                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(300),\n",
    "    MaxNormDense(100),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e50d89",
   "metadata": {},
   "source": [
    "In each iteration the fit() method will call the object returned by max_norm() which will scale weights in return. In addition to rescaling the weights we can define different constraints, for instance, we can constrain bias terms by setting the bias_constrait argument. \n",
    "\n",
    "* A Dense layer usually has weights of shape [number of inputs, number of neurons], so using axis=0 means that the max-norm constraint will apply independently to each neuron’s weight vector. If you want to use max-norm with convolutional layers, make sure to set the max_norm() constraint’s axis argument appropriately (usually axis=[0, 1, 2]). - [Géron, A. (2019)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39828f86",
   "metadata": {},
   "source": [
    "**Important documentations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0326ca7",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
