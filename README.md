# Deep Learning Fundamentals with Tensorflow

## ðŸ‘‹ Introduction

Welcome to my notebooks on Fundamentals of Deep Learning with TensorFlow. In this repository, you will find my notes on Fundamentals of Deep Learning in which I gathered the building blocks of most deep learning algorithms. I used mainly three resources for creating these notebooks, I took my notes from [GÃ©ron, A. (2022)]() and [Chollet, F. (2021)]() and combined them with [Deep Learning Specialization]() on Coursera. In each notebook, I also provide a bunch of new resources that I found helpful. These notebooks are more focus-on hands-on practice with also a little bit of theory provided. I think these notebooks would be most useful for those who have prior experience with the course on Coursera as well as those who have experience with the aforementioned books or some experience in deep learning. 

## ðŸ—Š Content

**Deep Learning Fundamentals 1:** The first notebook introduces the Perceptron and covers backpropagation and activation functions. Finally, a shallow neural network is implemented and experimented with different hidden layer sizes.

**Deep Learning Fundamentals 2:** In the second notebook, we continue discussing shallow neural networks and implement a neural network with L hidden layers from scratch. We also implement a similar model using Keras.

**Deep Learning Fundamentals 3:** In this notebook, we explore Tensorflow and Keras to learn about the basic functionalities of these libraries. We then do some basic practices on MNIST and Covid Dataset. Please refer to my Covid Data Analysis Repository if you would like to see how I generated the data used in this notebook. Moreover, we also use an MLP for a basic regression task.

**Deep Learning Fundamentals 4:** This notebook focuses on the vanishing/exploding gradient problem and discusses ways to address it such as Initialization and Gradient Checking.

**Deep Learning Fundamentals 5:** In the fifth notebook, we go on our discussion on the vanishing/exploding gradient problem and talk about activation function saturation. We then explore commonly used activation functions and talk about Batch Normalization.

**Deep Learning Fundamentals 6:** In the sixth notebook, we explore commonly used regularization and optimization techniques in Deep Learning and implement these techniques from scratch.

**Deep Learning Fundamentals 7:** In this notebook, we demonstrate how to use the techniques learnt in the previous three notebooks using Tensorflow/Keras.



